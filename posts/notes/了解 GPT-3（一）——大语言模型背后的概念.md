---
title: "了解 GPT-3（一）——大语言模型背后的概念"
date: "2023-03-09"
year: "2023"
abstract: ""
---

## 前言

最近我的推特时间线上充满了 chatGPT：有人拿奇奇怪怪的问题问它，有把Siri、小爱同学接上 chatGPT，然后发现这才是语音助手该有的样子，有用 GPT API 开发邮件文本润色器、网页翻译插件、故事生成器的。chatGPT 让普罗大众轻易接触到自然语言处理的前沿成果，如此神奇，不去了解它一下就辜负了五花八门的盛景。

chatGPT 是 OpenAI 开发的——在 GPT-3 系列大语言模型基础上经过微调（fine-tuned）后的——人工智能聊天机器人（chatbot）。

欲识 chatGPT，先访 GPT-3，计划用两篇文章，浅显地解释与 GPT-3 这个语言模型相关的概念，今后再遇到它们就脸熟了不是。 

第一篇讨论大语言模型（LLM，Large Language Model），第二篇讲 OpenAI API 的使用。

第一篇的大纲：
1. 语言模型是自然语言处理领域的产物，NLP 旨在让机器理解自然语言；
2. 语言模型是基于已有的统计规律作出预测的机器，建立统计规律需要训练，训练模型一要有大量数据集，二要基于特定的架构；
3. GPT-3 的全称，基于转换器的生成式预训练模型，中各个名词的意思；

## 1. 自然语言处理：让机器理解自然语言

自然语言处理（Natural language processing，NLP）是语言学、计算机科学和人工智能的交叉学科领域，探索计算机如何理解、处理人类自然语言，目标是让计算机像人类一样能够“理解”文本的内容，包括其中的语言上下文的细微差别。NLP 结合计算语言学、机器学习领域的技术，来创造能识别上下文、理解自然语言意图的智能机器。

机器学习（Machine Learning，ML）属于人工智能领域，研究机器如何从已有经验学习，执行没有事先编过程序的任务。机器学习的分支深度学习（Deep Learning）借鉴人类大脑的工作方式，构建人工神经网络。神经网络是由许多相互作用的神经元组成的大型网络，能在少量外界干预下执行极其复杂的任务。

神经网络需要学习（训练），学习的方式（learning paradigms）大致分为三类：监督学习（Supervised learning）、无监督学习（Unsupervised learning）和强化学习（Reinforcement learning），三种学习方式各擅胜场。监督学习通常用于模式识别、回归（regression）等类型的任务，适合处理有顺序的数据，如手写字、语音和手势的识别等。无监督学习通常处理估算类问题，如聚类、统计分布的估算、压缩和过滤等。强化学习通常处理控制类问题、需要连续做判断的任务等。

这些学习方式的具体学习过程不深入提及，只简单举一个监督学习的例子：为了让机器识别出一张图片是不是可爱狗狗图，需要准备好大量图片，人工标注哪些图片中有狗狗，哪些没有狗狗，然后交给机器，学习狗狗图的特定模式。学习完成后，再给新的图片，机器就可以判断里面有没有狗狗。这种学习的特点是，人们并没有把狗狗图的规律（四条腿、吐舌头、毛茸茸等）告诉机器，只是给它大量的狗狗图，由机器自己产生其中的模式，模式的误差率随着学习过程越来越低，趋于稳定后表示学习结束。

神经网络的理论和技术在 2010s 已趋成熟，在此基础上孕育出大语言模型（large language models）。大语言模型是由上万乃至上百万个——称作人工神经元（artificial neurons）——简单处理单元构成的稠密神经网络。

人工神经网络是自然语言处理领域的第一个里程碑，原先只在理论层面进行的复杂自然语言任务，通过训练模型变得可行起来。

自然语言处理的第二个里程碑是预训练模型（pre-trained），经过通用任务训练得到模型后，经过微调（fine-tuned）就可以继续训练不同类型的下游任务，不用重新开始，大大节省了训练时间。

现实生活中使用自然语言处理技术的应用有：

	- 电子邮箱的垃圾邮件检测功能
	- 机器翻译
	- 各种虚拟助手、聊天机器人
	- 社交媒体上的情绪分析
	- 搜索引擎的语义搜索（semantic search）功能

小结：自然语言处理关心的问题是如何让计算机理解、处理人类自然语言，理解尤其指语言中的上下文语境。机器学习领域对人工神经网络的训练实践将这个问题转变为训练语言模型的问题。

## 2. 语言模型：越大越好

从计算机角度来说，语言建模（language modeling）指为某种语言写的文本中一系列单词计算概率的任务。简单语言模型看到一个单词后，可以基于现有文本序列的统计分析，预测最有可能跟在它后面的单词、词组。一个能有效预测单词序列（word sequence）的语言模型，需要在大量数据集（dataset）上进行训练。

语言模型是所有使用自然语言处理的应用的要件，它是一个基于统计作出预测的机器，给一段文本，它能预测下一个符合统计规律的结果。它的实际应用如手机键盘上的自动填充（auto-complete）功能，打一个“你”字，它就、预测下一个词可能是“好”、“们”、“妈”。

使用模型进行预测是数学与计算机结合得到的威力强大的工具，《三体》电视剧的“三体游戏”世界中，一干科学家孜孜寻求一个能预测乱纪元和恒纪元的模型，周文王创造了一本万年历，用它占卜下一个恒纪元；墨子大量观察太阳运行规律，制造了一个宇宙机器，模拟未来的宇宙状态；牛顿在三定律基础上用微积分建立三体运动的定量数学模型，又用计算机计算未来太阳运行轨道的数据；爱因斯坦引入引力摄动，修正模型；最后人们发明高级算法，在百万年尺度上得到了的三体恒星、卫星的发展历史及未来运行结果。

GPT-3 作为一种通用语言模型，可以进行一系列任务的训练，在此之前的算法和架构下的语言模型只能处理某个特定的 NLP 任务，如训练一个模型用于文本生成（text generation）任务，训练另一个模型做文本总结（summarization）等。

GPT-3 如何具有这种能力？下面逐个考察名称中的每个词。

## 3.  GPT-3：Generative Pre-trained Transformer

GPT全称叫：基于转换器的生成式预训练模型，有三个关键词：
	1. Generative models，它是个生成式的模型；
	2. Pre-trained models，它是个预训练的模型；
	3. Transformer models，它是个基于转换器的模型

### 3.1 生成式模型

GPT-3 是个生成式模型是因为它能生成文本，生成式模型是统计模型（statistical models）中的一种，与它相对的是判别式模型（discriminative model），生成模型可以生成新的数据实例，判别模型可以区分不同种类的数据实例，如，生成模型可以生成看起来像真实动物的新动物照片，判别模型可以区分狗和猫。

训练一个模型，需要准备预处理过的数据集（dataset），帮助模型学习如何执行某项任务。数据集指某个特定领域内的大量数据，比如用数百万张汽车图片来教会模型什么是汽车。数据集也可以是句子或音频的形式。让模型学习了足够多样例后，就可以训练它来生成类似的数据。

### 3.2 预训练模型

为了得到一个表现良好的模型，我们需要用一些特定的变量对其进行训练，这些变量叫参数（parameters），训练模型就是确定模型的理想参数的过程，模型通过连续的训练迭代来吸收参数值。

一个深度学习模型要找到这些理想参数需要花费很长时间，根据任务的不同，或几个小时或几个月，耗费大量的算力。如果能将训练某个任务的过程，在另一个训练任务中重复使用，将大大节省时间，这预训练模型的用处。

这好比一个人花一万小时发展出弹钢琴的作为第一技能，之后再学习弹吉他，不用再花一万小时就能习得这个技能。预训练模型从通用的任务开始训练，再给它有针对性的数据集进行微调（fine-tuned）训练，得到的模型就能执行具体的任务。有了预训练模型，就不需要为具体任务从头开始构建一个模型，节省了重新发明轮子的时间。

训练 GPT-3 的数据集有5个：
	-  Common Crawl 语料库，OpenAI 在8年中的爬取网络数据收集到 petabytes 级别的数据，包括原始网页数据、元数据和文本数据，使用经过筛选的版本。
	-  WebText2：WebText 数据集的扩展版本，由 OpenAI 内部创建的一个语料库，通过爬取高质量的网页得到。保证质量的方式是，从 Reddit 上爬取的至少获得三个karma值（一个指标，用于标识用户是否认为链接有趣、有教育意义或仅具有娱乐性）的外部链接，包含来自这些 4500 万个链接、800 多万个文档的 40 GB文本。
	-  Books1 和 Book2：两个包含数万本不同主题书籍文本的语料库。
	-  Wikipedia：维基百科的所有英文文章，2019年确定使用，包含约 580 万篇英文文章。

以上语料库合集近万亿的文字数量。

![top 10 language](https://user-images.githubusercontent.com/20923112/224035167-9a2f9399-f77e-4d72-87d8-8c041f18641a.png)
图：GPT-3 数据集收录的前 10 种语言（来源：《GPT-3》 table1-1）

### 3.3 基于转换器的模型

神经网络是深度学习的核心，它模仿生物神经元通过传递信号的方式进行工作。神经网络的创新可以提高下游任务的模型性能，因此 AI 科学家不断研究神经网络的新架构，其中一个发明彻底改变了我们今天所知的自然语言处理：Transformer（转换器）。Transformer 是一种机器学习模型，它一次能处理文本序列（而不是一个单词），并有强大的机制理解单词之间的联系。

#### 序列到序列架构

Transformer 模型由 Google 在 2017 年发布的论文 `Attention is All Yuo Need` 中提出： 

> 我们提出了一种新的简单网络架构，Transformer，仅基于注意力机制，完全不需要循环和卷积。在两个机器翻译任务上的实验表明，这些模型在质量上更优，同时更易于并行化，并且需要的训练时间也少得多。

 Transformer 模型的骨干叫序列到序列架构（sequence-to-sequence，以下简称 Seq2Seq）。Seq2Seq 将输入的元素序列（如句子中的单词），整体转换为另一个序列，如另一种语言的句子；基于序列进行转换是因为单词顺序对于理解句子至关重要。
 
 Seq2Seq 模型特别适合翻译，因为从一种语言的单词序列转换为另一种语言的不同单词序列就是它的本职工作。Google 翻译在 2016 年底就开始在生产中使用基于 Seq2Seq 的模型。
 
 Seq2Seq 模型由两部分组成：编码器（encoder）和解码器（decoder）。我们将编码器和解码器想象成只会两种语言的翻译家，假设编码器的母语是法语，解码器的母语英语，且两者会说相同的第二语言：如韩语，为了将法语翻译成英语，编码器将法语语句转换为韩语（这个韩语叫上下文，context）并将上下文传递给解码器，由于解码器也能理解韩语，因此就可以将韩语翻译成英语。编码器与解码器通过合作完成了法语到英语的翻译，如下图所示：

![Seq2Seq model](https://user-images.githubusercontent.com/20923112/224035492-50a742d3-889d-4188-a49f-d93ce443df4d.png)
图：Seq2Seq 模型示例 （来源：《GPT-3》Figure 1-1）

#### 转换器注意力机制

与 Seq2Seq 模型紧密相关的一个概念叫注意力机制（attention mechanisms），注意力机制是种模拟人类注意力的技术：它把输入序列拆成一块一块，根据概率决定序列的哪些部分有重要语义。 例如这个句子 “The cat sat on the mat once it ate the mouse.”，句子中的 “it”指的是“the cat” 还是 “the mat”？Transformer 模型推断出 “it” 与 “the cat” 有强烈的联系，这就是注意力。 

如果编码器写下了对句子语义重要的关键词，并把它们与翻译一起传递给解码器。这些关键词让解码器知道哪些部分有重要语义，哪些术语给句子提供了上下文，从而使翻译更容易。 

Transformer 模型的注意力机制分两种：self-attention（自我注意力），关注句子内单词之间的联系以及 encoder-decoder attention（编码器-解码器注意力），关注从源语言句子到目标句子的单词之间的联系）。

注意力机制让 Transformer 过滤数据中的噪声，使 Transformer 根据上下文建立两个单词之间的语义关系，即使单词本身没有任何明显的标记指向彼此。

大型的架构与大量的数据让 Transformer 模型如虎添翼，在大型数据集上进行训练并进行特定任务的微调就能提升训练结果。相比其他神经网络，Transformer 更擅长理解句子中单词的上下文。GPT 只是 Transformer 中的解码器角色。

小结：转换器是一种机器学习模型，它按照序列处理文本，且能根据上下文识别单词之间的语义关系。GPT 是 Transformer 中的解码器。

### 3.4 GPT-3 简史

GPT-3 由人工智能研究先驱者 OpenAI 开发，OpenAI 的愿景是“确保人工通用智能惠及所有人”（“to ensure that artificial general intelligence benefits all of humanity）。人工通用智能指在各种不同类型的任务中都表现良好的人工智能。GPT-3 是人工通用智能的重要里程碑。

#### GPT-1 

GPT-1 于 2018年6月推出，当时开发者发现将 Transformer 架构与无监督预训练结合是一种充满前景的方式，他们认为 GPT-1 是经过微调的能实现“强大的自然语言理解”的模型。

GPT-1 是实现具有通用语言能力的语言模型的重要基石，它证明了语言模型也可以有效地进行预训练，使它们具备良好的通用能力。该架构经过微调就可以执行各种自然语言处理任务。

GPT-1 使用了 `Book Corpus` 数据集（包含约 7000 本未出版的书籍）以及具备自我注意力机制的 transformer 的解码器来训练模型。该架构基本与原始 transformer 相同。该模型有 1.17 亿个参数。 GPT-1 为未来的模型铺好了路，它们模型可以利用更大的数据集和更多的参数来释放其潜力。

GPT-1 的显著成就是在各种自然语言处理任务（如问答、情绪分析）中表现出的 `zero-shot` 性能， `zero-shot learning` 指模型在没有接触过任何相同类型任务的情况下，处理新任务的能力，`zero-shot task transfer` 是一种设置，在该设置中，给予模型极少或零示例，要求它根据示例和指令理解任务。

#### GPT-2

2019年2月 OpenAI 推出 GPT-2，与 GPT-1 非常相似只不过规模更大且可以进行多任务处理。

GPT-2 证明，更大的数据集上以及更多的参数，能提高语言模型的能力，超越许多 `zero-shot` 设置下的最新技术水平。它还证明，越大语言模型越擅长自然语言理解。

为了创建一个内容广泛的高质量数据集， GPT-2 开发者们爬去了 Reddit，从该平台上被点赞文章的外链中获取数据，由此得到的数据集 WebText，包含 800 多万个文档的 40GB 文本数据。GPT-2 在 WebText 数据集上训练，拥有 15 亿个参数，是 GPT-1 的 10 倍。

#### GPT-3

为了构建一个更强大的语言模型，OpenAI 构建了 GPT-3，它的数据集和模型规模比 GPT-2 大两个数量级：GPT-3 有 1750 亿个参数，基于 5 个不同的文本语料库进行训练。 GPT-3 的架构GPT-2 基本相同。它在 `zero-shot` 和  `few-shot` 设置下表现良好，可以编写出无异于与人类手写的文章。它还可以执行那些未事先训练过的任务，例如数学加减法，编写 SQL 查询语句，甚至能需求编写 React 和 JavaScript 代码。

OpenAI 研究人员惊讶于仅通过扩大模型参数和训练数据集的规模就可以带来如此非凡的进步，他们认为这种进步趋势将在规模大于 GPT-3 的模型上持续，届时，只需进行小样本上的微调，就能使 `zero-shot` 和  `few-shot` 下的学习模型越来越强大。

OpenAI 将 GPT-3 通过 API 发布，使全球用户可以通过简单登录就能使用世界上最强大的语言模型，这是革命性的举动。OpenAI 此举的一个意图是创建“模型即服务（model-as-a-service）”的商业范式，开发人员可以按 API 调用付费。

GPT-3 推出后引起了公众的关注，《麻省理工科技评论》将 GPT-3 评为 [2021 年 10 项突破性技术之一](https://www.technologyreview.com/2021/02/24/1014369/10-breakthrough-technologies-2021/)。

在 chatGPT 火热如潮的当下，OpenAI 研究人员估计正在构建基于万亿参数的语言模型。我们已然进入大型语言模型的黄金时代，每个人都可以成为其中的一份子。

## 4. 参考

1. [书籍：GPT-3 Building Innovative NLP Products Using Large Language Models](https://book.douban.com/subject/35852216/)
2. [维基百科：自然语言处理](https://en.wikipedia.org/wiki/Natural_language_processing)
3. [维基百科：人工神经网络](https://en.wikipedia.org/wiki/Artificial_neural_network)
